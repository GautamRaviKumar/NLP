{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dokj3AV9zI6Z"
      },
      "source": [
        "## Tutorial - Seq2Seq With Pretrained Embeddings\n",
        "\n",
        "We will learn how to improve the Seq2Seq model for NMT in Week 10 with pretrained embeddings.\n",
        "\n",
        "\n",
        "We will use the same dataset and Seq2Seq model with attention from Week 10. Let's start loading data and defining Seq2Seq model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWU237IjzI6e",
        "outputId": "20e17aae-4085-450e-df79-f9a7433c3212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp39-cp39-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.11.0) (4.5.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch==1.11.0 numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fZldjD8hzI6h"
      },
      "outputs": [],
      "source": [
        "## Requirements\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#replace the following path according to your Google Drive path\n",
        "#%cd/gdrive/My Drive/Monash-FIT-S1-2022/Basic-CYK-Parser\n",
        "folder_path = \"/content/drive/MyDrive/Monash-FIT-S1-2023/week9\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTnXQKQ55UaK",
        "outputId": "414b1001-c0fd-4ac3-9885-8e99eb3775e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8pfH3G-zI6l"
      },
      "source": [
        "#### Loading data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GIF75UoszI6o"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhpa39gqzI6q"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bCghv1VyzI6q"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JKaq7QGzI6r"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split lines into pairs. The files are all English → Other Language, so if we want to translate from Other Language → English I added the `reverse` flag to reverse the pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fG4twu-pzI6s"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open(folder_path + '/data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyAuHpoGzI6s"
      },
      "source": [
        "Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes ending punctuation) and we’re filtering to sentences that translate to the form “I am” or “He is” etc. (accounting for apostrophes replaced earlier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JFBL2updzI6t"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZgHx1ICzI6u"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "- Read text file and split into lines, split lines into pairs\n",
        "- Normalize text, filter by length and content\n",
        "- Make word lists from sentences in pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvic3S4UzI6u",
        "outputId": "520c2574-8649-407f-9bb9-96355ada4336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "['je suis desole mais vous devez partir .', 'i m sorry but you need to leave .']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pretrained Embeddings\n",
        "NMT is notorious for data hungry. Our toy dataset only has less than thousand training sentence pairs which is quite small. While parallel data may be expensive to obtain, monolingual data is abundant and often cheap (even free) to grab from Wikipedia, books, news. With monolingual data, we can train a language model and embedding and then initialize our NMT model with the pretrained embedding, or even the encoder and decoder weight to improve performance of NMT model."
      ],
      "metadata": {
        "id": "amxFc6_HBc4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will use English and French pretrained embedding from [MUSE project](https://github.com/facebookresearch/MUSE)."
      ],
      "metadata": {
        "id": "Rq7wd3cjBgXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.en.vec\n",
        "!wget -nc https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ8cOgYWBYdM",
        "outputId": "c4f26356-3835-4a2e-fe98-03034c4a6225"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-27 12:04:54--  https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.en.vec\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.8.35, 13.35.8.51, 13.35.8.29, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.8.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 628614720 (599M) [text/plain]\n",
            "Saving to: ‘wiki.multi.en.vec’\n",
            "\n",
            "wiki.multi.en.vec   100%[===================>] 599.49M  23.8MB/s    in 27s     \n",
            "\n",
            "2023-04-27 12:05:22 (22.2 MB/s) - ‘wiki.multi.en.vec’ saved [628614720/628614720]\n",
            "\n",
            "--2023-04-27 12:05:22--  https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.8.35, 13.35.8.51, 13.35.8.29, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.8.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 628737710 (600M) [text/plain]\n",
            "Saving to: ‘wiki.multi.fr.vec’\n",
            "\n",
            "wiki.multi.fr.vec   100%[===================>] 599.61M  24.2MB/s    in 27s     \n",
            "\n",
            "2023-04-27 12:05:50 (22.4 MB/s) - ‘wiki.multi.fr.vec’ saved [628737710/628737710]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The English dictionary contains 20000 words and the embedding size is 300.\n",
        "Our English vocab size is much smaller, similarly for our French vocabulary."
      ],
      "metadata": {
        "id": "Ae7lwgf5Bnnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_lang.name, input_lang.n_words)\n",
        "print(output_lang.name, output_lang.n_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50_Ijz2NBm7i",
        "outputId": "11ae017f-b8f0-43bb-8a42-83b100a78091"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fra 4345\n",
            "eng 2803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, we only need to load the embeddings of words appeared in our English and French vocabularies."
      ],
      "metadata": {
        "id": "9C3ywwxWCbaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 300\n",
        "def load_embeddings(embedding_file, vocab_lang):\n",
        "  embeddings = np.zeros((vocab_lang.n_words, embedding_dim))\n",
        "  with open(embedding_file, encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
        "    for line in f:\n",
        "        word, vect = line.rstrip().split(' ', 1)\n",
        "        vect = np.fromstring(vect, sep=' ')\n",
        "        if word in vocab_lang.word2index:\n",
        "          embeddings[vocab_lang.word2index[word]] = vect\n",
        "  embeddings = torch.from_numpy(embeddings).float()\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "6ieTOyWDCqVv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_embedding = load_embeddings('wiki.multi.fr.vec', input_lang)\n",
        "english_embedding = load_embeddings('wiki.multi.en.vec', output_lang)"
      ],
      "metadata": {
        "id": "C6Sv-SfeHYVq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrzMk443Iu5v",
        "outputId": "85c71644-9e59-44e4-eea9-dff4183fa728"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2803, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMvAasdlzI6w"
      },
      "source": [
        "#### Seq2Seq Model With Attention\n",
        "In Week10 tutorial, we set the embedding size with the same hidden size of the RNN for simplicity. Since we use MUSE pretrained embedding here, we have to set the embedding size to 300 - the dimension of MUSE embedding. We will modify the EncoderRNN and AttnDecoderRNN to allow embedding size having different value to the RNN hidden size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_YTIJriszI6w"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, pretrained_embeddings=None):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        \n",
        "        # load pretrained embedding if any\n",
        "        if pretrained_embeddings is not None:\n",
        "          self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
        "        else:\n",
        "          self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zVdYy0A8zI6x"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH, pretrained_embeddings=None):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # load pretrained embedding if any\n",
        "        if pretrained_embeddings is not None:\n",
        "          self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
        "        else:\n",
        "          self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
        "\n",
        "        self.attn = nn.Linear(self.hidden_size + self.embedding_size, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size + self.embedding_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_anYBECZzI6y"
      },
      "source": [
        "#### Preparing Training data\n",
        "To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WR_e0NCpzI6y"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiQI6_KUzI6z"
      },
      "source": [
        "#### Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uzyLpmtWzI6z"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW8pT7evzI60"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time remaining given the current time and progress %."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7gXZYsOczI60"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQyK8PaJzI60"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "- Start a timer\n",
        "- Initialize optimizers and criterion\n",
        "- Create set of training pairs\n",
        "- Start empty losses array for plotting\n",
        "\n",
        "Then we call `train` many times and occasionally print the progress (% of examples, time so far, estimated time) and average loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4NQNyNEozI60"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Fu80CIzI61"
      },
      "source": [
        "#### Plotting results\n",
        "Plotting is done with matplotlib, using the array of loss values `plot_losses` saved while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nQk3Npc6zI61"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "    \n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GstsNPB_zI61"
      },
      "source": [
        "#### Evaluation\n",
        "Evaluation is mostly the same as training, but there are no targets so we simply feed the decoder’s predictions back to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. We also store the decoder’s attention outputs for display later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zHQoRmIezI62"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Vu62-yzI62"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VnGhPPUnzI62"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attention= evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRTPZpKEzI62"
      },
      "source": [
        "#### Training and Evaluating\n",
        "With all these helper functions in place (it looks like extra work, but it makes it easier to run multiple experiments) we can actually initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small dataset we can use relatively small networks of 256 hidden nodes and a single GRU layer. After about 40 minutes on a MacBook CPU we’ll get some reasonable results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uZkDNYsszI63"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "embedding_size = 300\n",
        "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size, pretrained_embeddings=french_embedding).to(device)\n",
        "decoder = AttnDecoderRNN(embedding_size, hidden_size, output_lang.n_words, dropout_p=0.1, pretrained_embeddings=english_embedding).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainIters(encoder, decoder, 200000, print_every=5000)"
      ],
      "metadata": {
        "id": "7KHyCYFaJhe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "outputId": "c9630d1e-e9b5-49eb-a02f-3ca20018bbf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2m 13s (- 86m 30s) (5000 2%) 3.3265\n",
            "4m 0s (- 76m 4s) (10000 5%) 2.7641\n",
            "5m 50s (- 71m 56s) (15000 7%) 2.6103\n",
            "7m 43s (- 69m 27s) (20000 10%) 2.5056\n",
            "9m 35s (- 67m 8s) (25000 12%) 2.3741\n",
            "11m 26s (- 64m 49s) (30000 15%) 2.2859\n",
            "13m 17s (- 62m 41s) (35000 17%) 2.1516\n",
            "15m 8s (- 60m 34s) (40000 20%) 2.0691\n",
            "16m 59s (- 58m 30s) (45000 22%) 2.0025\n",
            "18m 49s (- 56m 28s) (50000 25%) 1.9284\n",
            "20m 42s (- 54m 34s) (55000 27%) 1.8649\n",
            "22m 32s (- 52m 35s) (60000 30%) 1.7732\n",
            "24m 22s (- 50m 37s) (65000 32%) 1.6925\n",
            "26m 12s (- 48m 40s) (70000 35%) 1.6220\n",
            "28m 0s (- 46m 41s) (75000 37%) 1.5656\n",
            "29m 52s (- 44m 49s) (80000 40%) 1.5034\n",
            "31m 47s (- 43m 0s) (85000 42%) 1.4551\n",
            "33m 38s (- 41m 7s) (90000 45%) 1.3840\n",
            "35m 31s (- 39m 16s) (95000 47%) 1.3167\n",
            "37m 24s (- 37m 24s) (100000 50%) 1.2952\n",
            "39m 16s (- 35m 32s) (105000 52%) 1.2548\n",
            "41m 10s (- 33m 41s) (110000 55%) 1.1776\n",
            "43m 3s (- 31m 49s) (115000 57%) 1.1205\n",
            "44m 58s (- 29m 59s) (120000 60%) 1.0838\n",
            "46m 50s (- 28m 6s) (125000 62%) 1.0494\n",
            "48m 42s (- 26m 13s) (130000 65%) 1.0447\n",
            "50m 34s (- 24m 20s) (135000 67%) 0.9823\n",
            "52m 25s (- 22m 27s) (140000 70%) 0.9490\n",
            "54m 16s (- 20m 35s) (145000 72%) 0.9079\n",
            "56m 7s (- 18m 42s) (150000 75%) 0.8875\n",
            "57m 57s (- 16m 49s) (155000 77%) 0.8440\n",
            "59m 49s (- 14m 57s) (160000 80%) 0.9106\n",
            "61m 39s (- 13m 4s) (165000 82%) 0.8876\n",
            "63m 29s (- 11m 12s) (170000 85%) 0.8435\n",
            "65m 21s (- 9m 20s) (175000 87%) 0.7668\n",
            "67m 12s (- 7m 28s) (180000 90%) 0.7950\n",
            "69m 3s (- 5m 35s) (185000 92%) 0.7527\n",
            "70m 53s (- 3m 43s) (190000 95%) 0.7115\n",
            "72m 45s (- 1m 51s) (195000 97%) 0.7543\n",
            "74m 37s (- 0m 0s) (200000 100%) 0.7912\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5wUVbbA8d+ZAEMYchCBYUAyCAKDqIgKGAgqq6hrTqzoM++6q5hzfmtaRZ+6rHnNAUVFURAFQUHJWXLOMIQZJpz3R1X3dJzpYap7Auf7+fChu+p21aFnuF1969x7RFUxxhhT+SWVdwDGGGO8YR26McZUEdahG2NMFWEdujHGVBHWoRtjTBWRUl4nbtSokWZmZpbX6Y0xplKaOXPmVlVtHGlfuXXomZmZzJgxo7xOb4wxlZKIrIq2z4ZcjDGmirAO3Rhjqgjr0I0xpoqIqUMXkZUiMldEZolI2MC3iFwkInPcNlNFpLv3oRpjjClOaW6K9lfVrVH2rQBOVNUdIjIYeBnoU+bojDHGxMyTLBdVnRrwdBrQwovjGmOMiV2sY+gKfCMiM0VkZAltRwBfRdohIiNFZIaIzNiyZUtp4jTGGFOCWDv041W1JzAYuE5ETojUSET643Tot0Xar6ovq2qWqmY1bhwxL75Eizdm89Q3i9m6J/egXm+MMVVVTB26qq5z/94MfAIcHdpGRLoBrwLDVHWbl0EGWro5m+e+X8b2vQfidQpjjKmUSuzQRaSWiKT7HgOnAvNC2mQAHwOXqOqSeATqPxcCgNXlMMaYYLHcFG0KfCIivvbvqOrXInINgKq+BNwDNARGu+3yVTUrHgE7hwfFenRjjAlU4hW6qi4H6rpt84Cz3O0vuZ05wFXAm0Btt11JN04PmvjjitcZjDGmcvIqD30w0M790wd4kTjlofuv0K1DN8aYIF5N/R8GvKGOaUA9EWnm0bFDuGPoNuRijDFBvMpDbw6sCXi+1t3mObtCN8aYyGIdcjleVdeJSBPgWxFZpKqTS3sy98NgJEBGRkZpX+4c46BeZYwxVZ9XeejrgJYBz1u420KPU+aJRW4WjV2hG2NMCE/y0IGxwKXiOAbYpaobPI+WgCwXG0M3xpggXuWhfwkMAZYB+4Ar4hNu0Ri6McaYYCV26G4eetj65gE56KiqAtd5G1pJcSXybMYYU/HFnLYoIski8ruIfBFhX4aITHT3zxGRId6GGXgu52/rz40xJlhp8tBvAhZG2XcX8L6q9gDOB0aXNbBoitZysS7dGGMCxVqCrgUwFGc1xUgUqOM+rgusL3to0YIpOqExxpgiseahPwPcCqRH2X8fzsSjG4BawMmRGnmZh24X6MYYEyyWtMXTgc2qOrOYZhcAr6lqC5xslzdFJOzYXuah2zW6McYEi2XIpS9wpoisBN4FBojIWyFtRgDvA6jqz0Aa0MjDOP3sCt0YYyKLZfnc21W1hapm4tzw/F5VLw5pthoYCCAinXA69LgUDbUsF2OMieygV1sUkQdE5Ez36S3AVSIyG/gvcLnGKQ3FKhYZY0xkpVkPHVWdBExyH98TsH0BztBM3BWttmg9ujHGBPJkYpG7/zwRWSAi80XkHe9CDDmP+7d158YYE6w0V+i+iUV1QneISDvgdqCvqu5wl9mND1sP3RhjIvJqYtFVwAuqugP8y+zGhVjFImOMiSjWIRffxKLCKPvbA+1FZIqITBORQZEaichIEZkhIjO2bDm4JBhLQzfGmMi8mliUglMg+iScSUaviEi90EaeTCzyHeugXm2MMVWXVxOL1gJjVTVPVVcAS3A6eM9ZxSJjjInMq4lFn+JcnSMijXCGYJZ7G6qjaGKR9ejGGBPIq4lF44FtIrIAmAj8Q1W3eRFg2Hndv+0K3RhjgsWctigiycDTuMWfQyYWKfA3EZkCfIhTii4ubOq/McZE5lWBC9xC0jcB08saVPGswIUxxkTiVR46wIPA40COB3EVE4vzt3XnxhgTzJM8dBHpCbRU1XHFHcSTPHTfA+vRjTEmSJnz0N1CFk/hrLhYLG8LXBhjjAnkRR56OtAVmOS2OQYYKyJZHscaxNIWjTEmWJnz0FV1l6o2UtVMt8004ExVnRGPgC1t0RhjIvMqDz1hxFZbNMaYiDwpcBHS5qSyBlUc32qLG3fHNZnGGGMqHU8KXIjI39ziFnNE5DsRaeVtmEWyc/MAuOvTefE6hTHGVEpeTSz6HchS1W44M0WfKGtg0ezLLYjXoY0xplLzZGKRqk5U1X3u02lAC2/CC5ebH21JdmOMObR5VeAi0Ajgq0g7vJhY1LFZOgDtm9Y+qNcbY0xV5VWBC1/bi4Es4MlI+72YWHREY6cjP7XzYQf1emOMqapiyXLxTSwaAqQBdUTkrdA10UXkZOBO4ERVzfU+1CKpyUKB5S0aY0wQTwpciEgP4P9wJhTFrUB0wPkotA7dGGOCeDWx6EmgNvCBiMwSkbGeRBdFsgiFhdahG2NMoFLloeMUuACciUWq6uu4h+JMOKoN5AI3ehhjmOQkwfpzY4wJ5lUe+ghgh6q2xen0Hy9rYMURgQLr0Y0xJohXBS6GAa+7jz8EBkoc17lNThKrWGSMMSG8ykNvDqwBUNV8YBfQsMzRRZEkluVijDGhPM1Dj+FYZZ5YBG6HbhNGjTEmiBcFLgDWAS0BRCQFqAtsCz2QFxOLALbuyeWPLXsO+vXGGFMVeZKHDowFLnMfn+O2ieuYyC8rtsfz8MYYU+mUaj30QCLyADDDTV38N/CmiCwDtuN0/MYYYxLIkwIXqpoDnOtlYMUZ3PUwG3IxxpgQsdwUTRORX0RktojMF5H7I7TJEJGJbgGMOe66L3GTmpzEAVtG1xhjgsRyUzQXGKCq3YGjgEEickxIm7uA91W1B85wy2hvwwyWmpxEXoGlLRpjTKASh1zcm5u+8Y1U909ob6pAHfdxXWC9VwFGUi0liQOWt2iMMUFinSmaLCKzgM3At6o6PaTJfcDFIrIW+BK4IcpxPMlDr5Ys5FmHbowxQWLq0FW1QFWPwiktd7SIdA1pcgHwmqq2AIbgZLyEHdurPHQbQzfGmHClWj5XVXcCE4FBIbtGAO+7bX7GKYTRyIsAI0lNSbIrdGOMCRFLlktjEannPq4BnAIsCmm2GhjotumE06Ef/JhKCaq5N0VtgS5jjCkSSx56M+B1dz30JJxsli9CJhbdArwiIn/FuUF6eTxnilZLcT6H8gqUailxW9TRGGMqlViyXOYAPSJsD5xYtABnzZeESE12OvEDBYX+zt0YYw51nkwsctudJyIL3DbveB9qkdRkJ2y7MWqMMUViGXLxTSzaIyKpwE8i8pWqTvM1EJF2wO1AX1XdISJN4hQvADVSk53A8gvieRpjjKlUvJpYdBXwgqrucF+z2csgQ9Wo5nTo+w5Yh26MMT5eTSxqD7QXkSkiMk1EQtMafcfxZGJRmnuFvt86dGOM8fNqYlEK0A44CWeS0Su+VMeQ43gysaime4W+P886dGOM8fFqYtFaYKyq5qnqCmAJTgcfF74x9Gl/hBVFMsaYQ5ZXE4s+xbk6R0Qa4QzBLPc00gC+MfR/fruEwkKbXGSMMRDbFXozYKKIzAF+xRlD/0JEHhCRM90244FtIrIA5wr+H6oat8tn3xU6QK6lLhpjDBBb2uISIA+n8xcgGcImFinwNxGZAnwILPM+1CK+K3SAnLyCoOfGGHOo8qrABSKSDtwEhGbAeK5matHn0Mpte+N9OmOMqRRK7NDVUVIeOsCDwONAjnfhRZZWrSjss0ZPjffpjDGmUvAkD11EegItVXVcCcfxqMBFcNhb9+Qe9LGMMaaqKHMeulvI4imcFRdLOo4neegiwqXHtvI/v+3DOQd9LGOMqSq8yENPB7oCk0RkJXAMMFZEsrwKMpL7z+zif7zFrtCNMabseeiquktVG6lqpqpmAtOAM1V1Rpxi9sXlfzxn7S427or70L0xxlRoXuWhl7tjHv2uvEMwxphy5UmBi5DtJ5U9rNgsenAQHe/+OlGnM8aYCs2TAhci8je3uMUcEflORFpFOpbX0lJtQpExxvh4NbHodyBLVbvhzBR9wtswjTHGlMSTiUWqOlFV97lPp+GkNybE6It6+h8v3ZSdqNMaY0yF41WBi0AjgK+iHMeTiUWBGtSq5n+8cKN16MaYQ5dXBS4AEJGLgSzgySjH8WRiUaDqKUX/hIYBnbsxxhxqvCpwgYicDNyJk4OesJk+1QI69IfHLWTQM5MTdWpjjKlQPClwISI9gP/D6czjWiA6VGrAui4LNuxm0cZs8gtsjXRjzKHHq4lFTwK1gQ9EZJaIjI1TvGGSJHxbdk5+ok5vjDEVRiwduq/AhRJS4EJVfR33UGASTqeeC9zoeaRRBC4B4LM7Jy9RpzfGmArDqzz0EcAOVW0LPI2zLnpCNEmvHrZt9367QjfGHHq8KnAxDHjdffwhMFAiXTrHQXpaKoseDL5Hu2u/XaEbYw49XuWhNwfWAKhqPrALaBjhOJ7noUNw6iJAdk4eT3+7hD+27InyCmOMqXo8zUOP4Tie56FD+Dj6xt05PPvdUi58ZZpn5zDGmIrOqzz0dUBLABFJAeoC27wI8GDc//kCALbtOVBeIRhjTMJ5kocOjAUucx+fA3yvqpEKSSdUfmG5h2CMMQnjVR76v4GGIrIM+BswKj7hlk1hofLYV4tYv3N/eYdijDGek/K6kM7KytIZM7yrUrfvQD6d7xkfcd/ZPZrz1J+PYvaanQx7YQpHZzbg/WuO9ezcxhiTKCIyU1Uj1myOZcilpYhMdAtYzBeRmyK0qSsinwcUwbjCi8BLo2a16MWXPv59HQAF7ofXAXdpgDXb97F2x76orzPGmMokliGXfOAWVe0MHANcJyKdQ9pcByxwJx+dBPxTRBK+9OHXN/eLum/H3gOs3hbcefd7YiLHPz4x3mEZY0xCxDKxaIOq/uY+zgYW4uSdBzUD0t3JRLWB7TgfBAnV8bA6Uff9uGwrN783C4DETHkyxpjEKlXaoohk4hSMDp1Y9DzQCVgPzAVuUtWwJQ/jNbEoFjf+93f/499X72SJVTcyxlQxMXfoIlIb+Ai4WVV3h+w+DZgFHI6z3svzIhJ2uRyviUWRdDwsvdj9w0dPjev5jTEm0WKd+p+K05m/raofR2hyBfCxu+7LMmAF0NG7MEvvtsHFnz47t2hEaNf+PEtlNMZUerFkuQhOnvlCVX0qSrPVwEC3fVOgA7DcqyAPRot6NWgcYSXGSHo/NIHjHvuez2evZ9Azk21xL2NMpRQ9169IX+ASYK67QBfAHUAGgKq+BDwIvCYic3HWTL9NVbfGId6YHdG4Nk3Sq7Mlu+RqeL40xhvccfajH57AUS3r8d7VlqtujKk8YhlyWYVTvCIFZ+nc/6jql6r6ktuZo6rrgUeAApwO/ar4hBu7pCThzqGdaFQ7tqv0QLn5hUxfsZ11NgxjjKlEPMlDd9d6GY1TU7QLcK7nkR6E445oxIsX9zzo1/d97HsPozHGmPjyKg/9QpyboqvddgktFF2c/IL4LW2gqlSANciMMQaIbQzdr5g89PZAqohMAtKBZ1X1DQ/iK7XPrutLg1pFk1QLPFhxce7aXfy2egeXHZcZtL33wxOoWS2Fybf2L/M5jDGmrGLu0EvIQ08BeuFkutQAfhaRaaq6JOQYI4GRABkZGWWJO6ruLesFPS8o4xX0sxOW8vQE55+R0bAm/Ts08e/buucAYGuuG2MqBq/y0NcC41V1r5vdMhnoHtookROLfAoKwyasloqvMwe44j+/snhjNmu224JexpiKx6s89M+A40UkRURqAn1wxtrLXWh5umfPP6pMxzvtmcn0e2IioyctK9NxjDHGa57koavqQhH5GpgDFAKvquq8eARcWie0a8wNA9pyZd/WpKUmU6NaMje9O6vkF5bgia8XexCdMcZ4p8oUuCiNeet2sWDDbm79cI4nx1v+yBBOfvoHhvdswXX923pyTGOMiSTuBS4C2vYWkXwROacsAcdb1+Z1OS+rJY8PP9KT442fv5HlW/by5PjFfPL7Wl6ZvJyCQuXuT+eROWqcJ+cwxpiSxDLk4ptY9JuIpAMzReRbVV0Q2EhEkoHHgW/iEGdc/Ll3BtVSkti+N48BHZvQ/38nAZCSJKUqML3vQIH/8V/fmw3Aw18W3UJQ1bCxfGOM8VqJHbqqbgA2uI+zRcQ3sWhBSNMbcDJhensdZDyd1aOF//E3fz2Bpulp1KqeTNs7v4r5GO/8srrY/bn5haSlJh90jMYYEwtPClyISHPgLODFEl5fbgUuYtG+aTp1a6aSkpzE5SGTiIozc9WOYvcv37K3jJEZY0zJvCpw8QzOCovFJn2XRx76wbqyb2s6NavDg8O6lPlYQ577kVd/XM7GXTkeRGaMMZHFlOXiTiz6AmfyUFguuoiswFllEaARsA8YqaqfRjtmeWa5lJaXNzZXPDrExtONMQetrFkuJU4sUtXWqpqpqpnAh8C1xXXmlc2jZ3uTDQPOCo4PfVF0+2HppmwyR41j2eY9np3DGHNoimXIxTexaICIzHL/DBGRa0TkmjjHVyG0a1I76HnDgMW/Smv9rhxe/WkFJzwxkfyCQr6YswGAsbPXlylGY4yJJcvlJ4qGU0qkqpeXJaCKKCuzAVNHDeDDmWvp1KwOd3wyt8zHXL19H1v25FI91flMXbtjH3PX7qJZvTR+WbGdIUc2K/M5jDGHFk8mFonIRSIyR0TmishUEQlbmKuyO7xeDW4c2I5TOjfltC5Nw/b3a9eo1Mc89tHvSUtx0hk//m0dZzz/EyNen8G1b//Gjr0Ht4rj/PW7yMkrKLmhMabK8aRiEbACOFFVj8SpL/qyt2FWLPed0YVxNx4ftK15vRoAPHJW6cbb8wqCE4Nmr9kJ4J/YtGb7Pjbvji07ZnN2DkOf+4k7Pi77NwhjTOXjScUiVZ2qqr5k7GlAC6qwlOQkuhxelz6tG/i39c50Hh9eL42XSlH27tGvFkXdt3VPLv2emMjRj3wX07H2uzNWf121PebzG2OqDq8qFgUaAUScZpmIAheJ9MplWXS7z1np4OyezenavC4dDkv3pLj0WaOnkNmwlv95vnslf//nC1i5bS9vjugT9prkJOdWRxmXgDfGVFJeVSzytemP06EfH2m/qr6MOxyTlZVV6Ytx1klL5a6hndibW4CI0OGwdCD4DvIn1x7HWaOnlvrYa3fsZ9ueonH0zveO54jGtVm4wXnrCwuVbxZs5NTOh/HDki10OCydJDe/Pd96dGMOSTF16DFULEJEugGvAoNVdZt3IVZsf+nXJmxbYcBkrR4Z9bljSEce+TL60Eo0+wNubh7IL/R35gDv/rqGOz6Zy6NnH8nt7pi5L18+noWxjTEVlycTi0QkA/gYuCS0juihKHTy7WldDvP8HMu3OBORApcTGDvLyWUvzUqRxpiqw5OKRcA9QENgtDutPT/a1NRDQWFIj56T5/0QyKs/rQg7157cfAAKrEM35pAUS4e+CpgENAUUeFlVvwxpcxXO+i1D3L9HehhjpZPRoCZX9m3NhX2cG78NyjCztCTZOfn+x7tz8oDwDr2gUNmTk0/dmqlxi8MYU/68ykMfDLRz/4ykhGV0qzoR4Z4zOtPWXTKgcXp1Fj04iCv6Znp+rtemrvQ/XrVtH+CMvWe7nTvAE18vovsD3zB7zU7yCwp5c9oq8gsK+XreRtZs3+d5TMaY8uFVgYthwBvqLN04TUTqiUgz97UGSEtNDhtbj6eTn/qBFy/uRePa1fm/ycsBGPbCFP/+goJC7vt8AbWrpzDv/tMSF5gxJm48KXCB08GvCXi+lpDJR+7rK3SBi3jLyqyfsHNt2p3L2aOncvq/foq4f687Cck37r5u537enLYqYfEZY7znVYGLmFSmAhfxcHq3w5l+x0D/8/E3nxD3c+7anxdxe2pyUbb857PXc9mYX7j703lsD1hD5r+/rObqNyvHmvXGGO/y0NcBLQOet3C3mRBN66QBUD0liQ6HpVO/Zio79kXudOMp8GbqDf/93f848Ibq7bYmjDGViid56MBY4FJxHAPssvHz6D645li+//tJAHwdcJXeM6Me4Cz0NWXUgLjG8K/vl0Xcnl9YyGlPT+ZzW5/dmErHqwIXXwLLgWXAK8C18Qm3auid2cC/OmPTOmmMOL41AM/8uQcAIkWrNwZ66rzuDO8Z33XP9uTks3hTdtBVO8C8dbssI8aYCs6TAhdudst1XgV1qLlraCfuHNKJnHznRuWVfVtHbHdG98PpndmAj35b69+WJODlPKJTnp4ctk1V/TdXVz42lLyCQgoKlbTUZDZn59AkPS3isZZuykYE2jZJ9y5AY0xUsQy5jBGRzSIyL8r+uiLyuYjMdgtgXOF9mFWbiJCUJNSslsLKx4Zy5fFFHXp6WgpX9WvNZce2IjU5iZYNavLFDcfTppGzEuPRAUv4xkvr24vmkW3ancOfXphCx7u/5vWpKzn64e/4Yk7k4ZlTnp7MyU+Ff0AYY+IjlpuirwHPA29E2X8dsEBVzxCRxsBiEXlbVQ+u5I7x+/HW/tSunkL9kJmmXZvXpXdmA5Zv3UudtMTO/uwTsDb7vWPnAzBz1Q5O73Z4QuMwxoSLpcDFZKC4igkKpLs3T2u7bfOLaW9i1LJBzbDO3Of+YV345NrjGNotuPbo9f3b0r1lvUSE57dm+z6+mb+RS8f84s9rN8YkXqkmFkXxPNAJWA/MBW5S1YirUR3qE4u8lJaaTI+M+gw7qjnf33Ii5/ZybpY2Tq/Oy5f0SmgsExZuZuSbM5m8ZAtd7x3P5f/5JaHnN8Y4vOjQTwNmAYcDRwHPi0idSA0P9YlF8dKmcW36tGkIQPum6f5cd4DXrzw64fFMWhz8YX3ZmKIO/oWJy5iwYFPYa6587Vf+8cHsuMdmTFVWqhJ0UVwBPOZmuiwTkRVAR8Au0xJoeM/m9M6sT6uAsnUAvVoVLTfw0f8cy/AXf05IPIEpjj8sKergnxy/GHBWoMzLL2TUkI5c1KcV3y/a7Ow/t3tC4jOmKvLiCn01MBBARJoCHXBy0k0CiUhYZw6QLEUZp4lcJr3fExODnheGnHz73gNk5+Zz5ycRk6eMMQchlrTF/wI/Ax1EZK2IjAiZVPQgcJyIzAW+A25T1a3xC9mUhgg0ql0dKCpNl4hUx1A79+fx3cLwoRaAfQci30jdsfcAd306l9z8goj7jTHBYhly2Q8kA4tVtWvoTlVdLyKPAM8AqTjFLt7yNEpTaiJOKbzkJOHbv57Atr0H2LTbKVeXkiRkNKhJksDKbYmZ/Xn7x3MYPz9yh37bR8FrxuTkFfDWtFUs3pjNBzPX0r1FPc7NahnxtcaYIrEMubwGDIq2U0TqAaOBM1W1C3CuN6GZsviz2wEmiVC/VjXaNqntrzWanCR8f8uJfH/LSUy/YyBZ7ji7b7JSPMxbF32BzqWbsoOej564jIfGLeSDmc6M2H98OIclbpsNu/Yzxi2/Z4wJ5kUe+oXAx6q62m2/2aPYTBk8fNaRzL3vVJKTisbQCwqdbNKUJCElOYmkJKFpnTRGX9ST/znpCH9Oe6dm4UlKz1/Yo0zxrNu5P+q+0KLWkZb8/cjt3K9+cyYPfLHA1pUxJgIvboq2B+qLyCQRmSkil3pwTFNGyUlCesgsUt8YenJS8I+9SZ00bhvUkRR3e98jGgbtX/zQoLjOBF22eY//8W+rdyASvnTQhIWbGPPTCuas3QXAgQLvC28bU9l50aGnAL2AoTg56XeLSPtIDW1iUfk6rm0jjmpZj1sHdYi4v24N55ZK0zppLHlosH979ZRkwOnY4+3s0VMjbv9jy14e+KKo6mFoIWxjjDcd+lpgvKrudbNbJgMRk4ltYlH5ql09hU+v60v7ppFXP7z4mFbce0ZnLu+bSbWU8F8NX8cOcHKnJnGL85PfS66NcurTk4OGcb6et5HV7g3eH5ZsYXdObEVDNu3OIXPUOGav2XlwwRpTgXgxsegznNmhKUA1oA/wtAfHNQmWkpzEFVGW7vUZeUIbNuzKoV6N+C0KFq1sXqi+j33PRX0yWL9zPxPd2anN69Vg3c79nNShMa9dUfIs2R+XOhm2r/+8kqdaHnXQMRtTEZTYobt56CcBjURkLXAvTnoiqvqSqi4Uka+BOUAh8Kqq2myRKuCyY1vRKzM4Z/2OIZ0AuPezivEjfnv66qDnvqv2FVv38seWPdz87ize+ksf6kb5AHImOBtTNcRS4OKCGNo8CTzpSUSmwrh/WNi0Az/fjcv6NVOplpJEbn4hO/fl0bZJ7aCbnCd3asKEhYlPfEoSYehzP5KTV8jERZv5U4/mCY/BmESL5Qp9DHA6sDnSxKKAdr1xZpSer6ofeheiqYjquFe8tw7qyAVHZwCweXcOX83b6F8nHSKX0kuEFVv3+h/f/N4snhy/mLZNavO/53ancXp1/75IGTXGVFZlnlgEICLJwOPANx7EZCqBa086gtsHd/Qv2wtO+mNKclEH2b9DY/56SsSEp4Rbt3M/PyzZQu+HJ/grLM1as5Ns9+bpluxcMkeNY9Azk/lo5lrGhhTJHvDPSQx97kfa3D6OXftiG+M3JtG8mFgEcAPwEWCTig4RaanJXH3iEaQkB/8KBS4G9tBZR1KvZlGBjmUPD+a8rBZMHTUgYXFG8t3CzeTmF/CnF6Zw/+dOKqTv5uiijdnc8sFsbgwpkr18y17mr99NocKCDdFnvZpD26TFm4udRBdvZU5bFJHmwFnAizG0tTz0Kq5v20b+xzVSk4P2pSQn8cQ53Tk8ZBjmxgFtg55nNqwZvwCB7Jx8+j0+seSGUUxfsY2Zq3Zw96fz7KaqCXL5f37ltAiF1hPFizz0Z3BWWCxx6p7loVd9LRvUZOnDg5n095No4JbPG3N5VsTJTL1a1WflY0P94/E+t7uZNPEyYeEmNmfnlthu/PyNEbc/M2Epw1+cypvTVrFhV47X4ZlKrjzLMHqRh54FvOveXGoEDBGRfFX91INjm0ooNTmJzICFvgZ0bMqAjk2D2sy571Squ5OXQtdySRahTloKu3PKtz7p1W/OBODBYV2ittm1Py/sG0eoNewAjvEAABgMSURBVNv38cyEpTw2/EhSk724hjImsjL/dqlqa1XNVNVM4EPgWuvMTUnqpKX6Z55eckwrLjmmFZcflwlAh8PS+eKGfvzrguAFwd7+S59EhwnA3Z/Nj7ovcAmCsbPX0+vBb8krKOSFicv8q0je9tEcPvptLb+sCL8V9eXcDfz8x7Ziz38gv5CZq0q6jQW79uWFFRIxhxYvClwYUya1qqfw4J+6cseQTkz8+0m0bFCTjIY1OaN70YJg/7qgB33bNuLHW/vz4639ub5/22KOmDiFAWPo942dz7a9B9icncuT4xdzytOTyc7JI3CYPTe/gM9mrfOPvV/79m9c8Mq0Ys/x0LgFDH/xZ6Ytj97xb8nOpfsD3/D8xGVl+weZSi2WLJcLVLWZqqaqagtV/bc7Q/SlCG0vtxx0c7CqpSTROsqa7F0Od5b0bdmgJi0b1OSqfm0SGVpUkRYJ2x9QgemqN2b4Hwtw96fzuOndWf5VI2Mxd53T9vyXo3f8m7Odsfwv526I+bim6onlCn2MiGwWkYhzvUXkIhGZIyJzRWSqiFiVX+OZu4Z24pg2DWjTuHbQ9uqpzq9ucpIw/Y6BnNK5aaSXx92B/EKmu1fO2/ceAIJXjFy2eQ9KUaf//oy1EY8z0u34Cwo1rCRfLMMoSW66qCXdHNq8mFi0AjhRVY/EqS/6sgdxGQPAX/q14d2Rx4Zt991Qvfy4TJrWSWPU4I6JDg2Ah79cyJ9fnsbQ5370bwu8mXsgv5AF68Pz1qct30bmqHH+598scMrzHfvod3S+Zzy/riwaMy+IoZf2d+iUvUd/ZfJy5q+P/RtEaT33nZMlVJV1uOurcjlvmScWqepUVd3hPp0GtIjW1hiviAh/PDKEu4Y6KY6ZDWMvn3f1CcHDNbWqJYe1SUmKbUkA39DJ/AidNjidu6+DD1zS963pq8LaDn72R3865ZRlRXXWA2t5rNu5n8xR45i8JPI8Di/uiT785UKGPvdT2Q8UxVPfLmHmqh0lN6xkAuck5OaXTwEWr3OoRgBRP5psYpHxUnKS+NdiSS6hAw6cvHR2z+BrjjOPOpzFDw3i9SuLltvNaOD95KZr3vrN/3jN9vDZhAsDZqAuWL+bv743iz25+UFDLovcNq+G1FXNc3v90ma5lHdWTE5eAS9MXMaB/EJ25+SxbU/J8wO8sHl3jue1aUO/SJXHpDPPOnQR6Y/Tod8WrY1NLDKJELi+DMADw7rwt1OdiU0DOjYhNBW8VcNaVE9J5vi2jRjU5TAAzuvd0r9/7PV94xtwBN8s2MQnv6/j3V9Wk19YdLU3bo5z03Pyki0s2uh07qrqL6K9dkfs086/W7iJNnd8yaKNu5m3bhczV+1IaAdfWKj8+6cVPDl+MW9NW8Wxj3xHr4cmJOTc1779Gw98sYDlW/aU3DhGoe9ceVylezGxCBHpBrwKDFbV4pNqjYmjNo1q8eS53XlseDfW79zPiq176dfOWY7glzsHUictlS3usMaJ7RtzcuemXOB23slJwkuX9CK/oJDkJOGxrxYBTl78d7ecyIcz1/LipD8S+u/5fM6GoIlXHwdUcxr0zI/0yKjH76uLqi0VV2v1+0WbuPI15+brb3efwtfznJmws9fs5LaP5gKw9OHBUV/vtTFTVrDXnVW570A+ew8URGw3Y+V25q3bxeUlFF8pDd/wV16Bdx9goVfk+w8UkJYaPpwXT2Xu0EUkA/gYuERVl5Q9JGMOzi93DKRmdedXOjlJ/CmOPk3S0wAn9fH/LunFMW0aRix84Vtw7P2rj+Xj39ZSLTmJIxrX5rZBHRPeoZdUGi+wM/fpcNdXJImw8MHgXIaHxi30P561Zof/ZmvgEsKzEliKb/LSrRzZ3ElHLW504pyXfgbwtEP33UQu9HBYJPTLzf68Aup7dvTYlLliEXAP0BAY7f5i5KtqVrwCNiaaJnXSYm57mju0UpyjWzfg6NYNIu7LaFCT1dv3BW2rWS2ZfVGuMhMp2lf9wOGUJBF/Jxq4Qua5bufps3PfAeav3x206FpZ1UhNZn9eAZOXbKGZ+zNL9Giz70PMy2LjoRlG5fG7EMsY+n4gGVgcZWLRVcCbQG33eCPjE6ox5e/7W07knb/04cNrwlMpyyuzIZrr3i66CZtXUBg0pLF4Y7a/GHdxN5SvfO1XLnp1Ovs97JyObFHX//i9GWsA+HBmUX5+aB5+PPjuo3h5hR56KC/fs1h5kYc+GGjn/hlJDMvoGlNZtWlcm+PaNqJJnTRWPjY0aJ+XV3teGDd3A29Nc9IjL3xlmv/eAcDPAcsI3PzerKjH8K39XqhKbn6B/0ZsWaQmh3+ABH7bmb0mfjnwPkVDLt4dM6xDz6uAHXoMBS6GAW+oYxpQT0SaeRWgMZXBub1a8NaIyIuHzb3vVMqr0t1dn87jhv/+zq8rg/O+Jy0uOW34hyVbyMlzvnUUqNLhrq8Z9MyP/mUGDlZKUvHdTrRvDHnF3PAtLQkYQ9+x9wBz1pb93kH4kEviVwv1Im2xObAm4Plad1sYy0M3VdWT53bn+HZF48yBV+/paaksfGAQ8+8/jZcu7pXw2D4PKacXq8vG/OJ/nBWQTrh7fz7vz1jDyoC6rQBvT19Fn0dKTjss6cMt2gjQGz+vKlVa5bx1u6J2qr4vCTNX7uCCV6Zx5vNTypw3HhpaTkW8QveS5aGbQ1VaajK1qqcwqGvJN2OL4yvInWgHAu4P5OYXcOuHcxj2whT25OYz9Y+tvD9jDXd+Mo9Nu3Npd+eXbNjl5MO/+uNyMkeN4/GvF5Gd4yzvW1KffKCgkHNenErmqHFs2l30beDBLxbQ5o4vw3LHc/IKuG/s/KCZuDl5BZz+r5/4n4DJXD7rdu5nm7vuzsNfLmTRxmz/ecsi9ANh3rqi4akj7xvPde+Ex+I1Lzr0dUDLgOct3G3GHHKuPqENjw8/EoDPrz+eMZeHJ3y9ETAjNdQNA9ryTjHrvl/Up3w69EB//2AO4BT36HrveC58ZTq3fjjHvz+vQP1DPL5UyRcn/cGR933Dla//WuJV9oWvTGeGuzTAMxOWhu3/0wtTgp4/M2Epr01dyTPfFrX13c8IXELBp+9j37Nq276w7WW9iRn6rwpcyjg7J98/KSyevJhYNBa4XkTeBfoAu1TV1vA0hwxfqT0ILp/nZHPUDWt/QvvI306fGN7NP0N15WNDgxbvOqF9Y0YN6khndxnhaIb3bMFHv0Ve0dErC2Mokp2bVxAxpz2WsftA//1lddi23Tn5vPvLajIa1KR141q89IMzNyBwjN2XvRJaDev9GWuIZu+BAuqVYcWHSEU4CwuVpBjXBfKCF3noXwJDgGXAPuCKeAVrTEUz7/7TgvK4D9Z1/Y/grJ7Bt56u7NuaMVOc9UYE/J35FzcczzMTljJh4aag9rPvOZWXJid24lM0/wi4Yo+ka/M6QUMSpTXqY2dma8OAD9PAm5KB/fgbP6/klM5NWbF1b9A3iVD3fjafVy8reQrN1D+2sml3Dmf1CF5iItJKl98v2szyrd4tL1CSEjt0Vb2ghP0KXOdZRMZUIrWre7J6Bv84LXz537tP78Q3Czaydsf+oK6ia/O6jL6oJ+1DlmitWzM1LK/6sDppbNxd8QpZp1dPpVHtamzdc6BMx/GNhQf6au4G/icgB/+ez+Yz5qcVrIwwzBIo9AMymgtfmQ7AsO7NyckvYG9uAY3Tq0e8N/CXgAIniRDTGLqIDBKRxSKyTERGRdifISITReR3t9jFEO9DNaZqatukdsTtIsKDw7pG3FctJYmVjw1l4t9PCtpeJy14KYORJ0Su7NQnygzYRNmx74A/JdIrvs+yuz4Nr8VTUmfuM2/dLqYs28qAf07i1R+X+4uXRFKgypnPT6H3wxPc85f/PIRYKhYlAy/gTCDqDFwgIp1Dmt0FvK+qPYDzgdFeB2pMVfTJtcfx/tXhs059jj2iIYO7HsYDZ3aJuL91o1r8csdAfr3zZAD+0q81owZ3pEdGPd4a0Ycr+mZydGaDsHNUT/CiUaH2HsjnkbOPpGWDGky/Y6Anx3x7+mpy8goiXrXHau66Xdzz2TyWb9nLQ+Oc4iXPf7+UzFHjgjJ9wLnxumxz0XBK+XfnsV2hHw0sU9XlqnoAeBdnMlEgBXx3a+oCB5f4aswhpkdG/aCbqqHSUpN58eJeZEaptQrOGjaN06sDUD0lmWtOPIJPru3L8e0aISK8f82xHN26AU8M7+Z/TaQi27WqJTP5H/3L8K+JXUpSEmd2P5wfbx1A0zppNKod/T0ojfd+jX7TMxa5eQVhMz7/9xtnzcG9ufl8FzAss3xLcB5+LMsIdL13fFyrGcXSoccyceg+4GL3pumXwA2RDmQTi4wpP+f1bskXNxzPd7ecSNfm4dkyzevXIKNhbGkemQ1rMvb6vhGn8ccidHjisbO7RWlZOveOnV+m1y/ZvCfqlfam7BxGvF40Jj4koOwgwE9Lw1MkQ+3JzSc3v5DnvgtPx/SCN3d04ALgNVX9p4gcC7wpIl1VgxN5VPVl3JqjWVlZFeEbijHl4q0RfVi3M7ZxXS91bV6URrnysaEs3phNeloKs9bsJKuVs9jr6d2a8UVIzvR/ruhN/w5N/KmUk9wr+av6tWF0DEsKt2xQg9aNakctnXdyORX5DvXO9PA0SZ9Bz/wYdd/MVdv52/uzYz7PYrcgiddi6dBjmTg0AncBL1X9WUTSgEbAZi+CNKaqCVwmoDx1OCwdgMPr1fBve/7Cnjx5TgGd7vkagKmjBgTtD7wqj3ZDN1D/Do159bLeXPHar/5tsVzN/efy3sxYtZ28AqVBrWo89tUiGtWuzmXHtuKf31as0gvDXyxadnj0RT0pVOX6d36P2j7ShCcvxDLk8ivQTkRai0g1nJueY0ParAYGAohIJyANsDEVYyqpGtWSueDoltwxpGNQZ/7LnQOZcecp/udn9QgefW1YqxrPnn8U957RmfZNnc7+r6e0JzlJ/N8AAM7Lakk0j559JEc2r0v/jk34x2kduWNIJ852z9OtRV1uGNjO37aJe++gImlWN43Tux1ebJtXL41PyYhY8tDzReR6YDzOuuhjVHW+iDwAzFDVscAtwCsi8lecD9/LtSLk8BhjDtqjEca1fVWffESEZ88/imcnLOWOIZ3odHgdmrsfAD8t3cqSTXv8ZdhGHN+avm0b0uXwulRPiX4tecHRGWFr1jSpk8YbVx7NURn1AOjesp5TOm9QR275IPahjkSoVsy/DWDKqAH+98hrsY6hF+J01AoUAKjqPb6dqrpARJ7FuTmqwOXAN14GaoypmIYd1ZxhR4UvsPrUeUfxzYKNtG/qDOvUqp5Cr1bR899fvTSL+sVk/AQumTDmsizGzd3A8F4teH/GGqavKFrhu1OzOsUuT/DUed1LNd5dWtVTnA+wT649jqWb94TNTo1XZw4e5aGLSDvgdqCvqnYBbo5DrMaYSqRuzVTOLWZoJdTJnZvSq1VsVTgb1q7OpcdmAtDRvQ8AcMkxrfjqpn5h7f/33O7+x2f3bMGse07ht7tPYdrtA7nmxCNijjEWvnsMPTLqBw0tJScJLerHrzOH2K7Q/XnoAO4iXMOABQFtrgJeUNUdAKpqN0ONMQlx59DOnNL5MPq0aUCKuxDW7YM78uhXi/xthvdszsCOTfxrsderWfRNYNTgjv4Fvl66uBeFqlz7dvBSt3cO6cTstTvDsn8iEYJTOb+8sR+pyUKrhrXiXugklg49Uh566Pqe7QFEZArOOPt9qvp16IFEZCRuzdGMjPJfBtQYU/lVS0kKyxoKHboRkWKHcwCGdmvmX6/+rB7N+eT3dQzqchhXndCGXq3qU1ioZOfk80NA6uXLl/Ri5Jszg44Tmstf0gqZXvKqwEUKTk3Rk3By0l8RkXqhjazAhTEmEXpmOEM3r16aFVb7NZIVjw7h+Qt6+J//pV9rAM7p1cI/DJSUJLx+5dGc28tZZbFF/Rqc2uUwzu8d+7BSvHmVh74WmK6qecAKEVmC08H/ijHGJFjbJrVj6sh9JGQspMvhdVny0OCIGStPnNONlg1qcno3p3SyL3Wyb9uG3D64U1j7RIqlQ/fnoeN05OcDF4a0+RTnyvw/ItIIZwhmuZeBGmNMIkVLPxQRbgzIhb/mpCPYn1fAzSe3p5ZHyykfLK/y0McDp4rIApy0xn+oavR1J40xpoqoWS2FO4eGLkBbPqS85v9kZWXpjBmJXfzdGGMqOxGZqaoRp5p6UuAioN1wEVERic+8VmOMMVF5VeACEUkHbgKmex2kMcaYknlV4ALgQeBxoOIVMDTGmEOAJwUuRKQn0FJVxxV3ICtwYYwx8VPmiUUikgQ8hbPiYrFsYpExxsRPLB16SROL0oGuwCQRWQkcA4y1G6PGGJNYZS5woaq7VLWRqmaqaiYwDThTVS0n0RhjEsiriUWlNnPmzK0isupgXotT3i4+NZzKpqLGBRU3NourdCyu0qmKcbWKtqPcJhaVhYjMiJZYX54qalxQcWOzuErH4iqdQy0ur1ZbNMYYU86sQzfGmCqisnboL5d3AFFU1Lig4sZmcZWOxVU6h1RclXIM3RhjTLjKeoVujDEmhHXoxhhTRVS6Dj3WpXzjdO6WIjJRRBaIyHwRucndfp+IrBORWe6fIQGvud2NdbGInBbH2FaKyFz3/DPcbQ1E5FsRWer+Xd/dLiLynBvXHHctnnjE1CHgPZklIrtF5ObyeL9EZIyIbBaReQHbSv3+iMhlbvulInJZnOJ6UkQWuef+xFefV0QyRWR/wPv2UsBrerk//2Vu7GWqLx8lrlL/3Lz+/xolrvcCYlopIrPc7Yl8v6L1DYn9HVPVSvMHZ2LTH0AboBowG+icwPM3A3q6j9OBJThLCt8H/D1C+85ujNWB1m7syXGKbSXQKGTbE8Ao9/Eo4HH38RDgK0BwlmqYnqCf3UacSREJf7+AE4CewLyDfX+ABjilFRsA9d3H9eMQ16lAivv48YC4MgPbhRznFzdWcWMfHIe4SvVzi8f/10hxhez/J3BPObxf0fqGhP6OVbYr9FiX8o0LVd2gqr+5j7OBhYSsPBliGPCuquaq6gpgGc6/IVGGAa+7j18H/hSw/Q11TAPqiUizOMcyEPhDVYubHRy390tVJwPbI5yvNO/PacC3qrpdVXcA3wKDvI5LVb9R1Xz36TSc9ZOicmOro6rT1OkV3gj4t3gWVzGi/dw8//9aXFzuVfZ5wH+LO0ac3q9ofUNCf8cqW4de4lK+iSIimUAPigp6XO9+dRrj+1pFYuNV4BsRmSkiI91tTVV1g/t4I9C0HOLyOZ/g/2jl/X5B6d+f8njfrsS5kvNpLSK/i8gPItLP3dbcjSURcZXm55bo96sfsElVlwZsS/j7FdI3JPR3rLJ16BWCiNQGPgJuVtXdwIvAEcBRwAacr32Jdryq9sSpLHWdiJwQuNO9EimXHFVxFnU7E/jA3VQR3q8g5fn+RCMidwL5wNvupg1Ahqr2AP4GvCMidRIYUoX7uYW4gOCLhoS/XxH6Br9E/I5Vtg69pKV8405EUnF+YG+r6scAqrpJVQtUtRB4haJhgoTFq6rr3L83A5+4MWzyDaW4f29OdFyuwcBvqrrJjbHc3y9Xad+fhMUnIpcDpwMXuR0B7pDGNvfxTJzx6fZuDIHDMnGJ6yB+bol8v1KAs4H3AuJN6PsVqW8gwb9jla1DL3Yp33hzx+j+DSxU1acCtgeOP58F+O7AjwXOF5HqItIaaIdzM8bruGqJU9MVEamFc1Ntnnt+313yy4DPAuK61L3TfgywK+BrYTwEXTmV9/sVoLTvz3jgVBGp7w43nOpu85SIDAJuxVmGel/A9sbi1PhFRNrgvD/L3dh2i8gx7u/opQH/Fi/jKu3PLZH/X08GFqmqfyglke9XtL6BRP+OleXObnn8wbk7vATn0/bOBJ/7eJyvTHOAWe6fIcCbwFx3+1igWcBr7nRjXUwZ76QXE1cbnAyC2cB83/sCNAS+A5YCE4AG7nbBKfz9hxt3Vhzfs1rANqBuwLaEv184HygbgDyccckRB/P+4IxpL3P/XBGnuJbhjKP6fsdectsOd3++s4DfgDMCjpOF08H+ATyPOwvc47hK/XPz+v9rpLjc7a8B14S0TeT7Fa1vSOjvmE39N8aYKqKyDbkYY4yJwjp0Y4ypIqxDN8aYKsI6dGOMqSKsQzfGmCrCOnRjjKkirEM3xpgq4v8BnfXDSIN8slsAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym6qoZjLzI64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff1b9ba-eb8b-4e2b-939b-937a2135f76e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> vous etes fort courageuse .\n",
            "= you re very brave .\n",
            "< you re very brave . <EOS>\n",
            "\n",
            "> il est photographe professionnel .\n",
            "= he s a professional photographer .\n",
            "< he s a teacher photographer . <EOS>\n",
            "\n",
            "> vous etes tres intelligentes .\n",
            "= you re very intelligent .\n",
            "< you re very timid . <EOS>\n",
            "\n",
            "> nous sommes en train de le reparer .\n",
            "= we re fixing it .\n",
            "< we re fixing it . <EOS>\n",
            "\n",
            "> elle porte un chapeau sympa .\n",
            "= she s wearing a cool hat .\n",
            "< she s wearing a cool hat . <EOS>\n",
            "\n",
            "> c est un pauvre type sans c ur .\n",
            "= he s a cold hearted jerk .\n",
            "< he s a cold hearted . <EOS>\n",
            "\n",
            "> je ne suis pas encore prete .\n",
            "= i am not ready yet .\n",
            "< i m still ready yet . <EOS>\n",
            "\n",
            "> je ne suis pas medecin mais enseignant .\n",
            "= i m not a doctor but a teacher .\n",
            "< i m not a doctor but an teacher . <EOS>\n",
            "\n",
            "> nous sommes prets desormais .\n",
            "= we re ready now .\n",
            "< we re ready now . <EOS>\n",
            "\n",
            "> nous sommes toutes dingues .\n",
            "= we re all crazy .\n",
            "< we re all insane . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder, decoder)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}